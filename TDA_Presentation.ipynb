{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topology of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the topology of deep neural nets in binary classification tasks:  \n",
    "1) How is a simple neural net built?  \n",
    "2) Binary classification  \n",
    "3) Topology of neural nets  \n",
    "4) Analysis of Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural net is given by the composition of functions of the form $f(x) = K(Wx + b)$, together with some scoring function.  \n",
    "With $K$ being some non-linear activation function, the weight matrix $W$ and a bias vector $b$.  \n",
    "In the picture below, each hidden layer corresponds to one of these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/intro_2.png \"Simple neural net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification tasks, we try to say to which of the classes of a given set, a picture corresponds.  \n",
    "In binary classification, we have only two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Topology of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below gives an inuition of what is meant by the topology of a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/intro_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given two disjoint manifolds (green and red)\n",
    "- Each corresponding to a certain class (e.g. red are all cat images and green are all dog images in our train set)\n",
    "- Each step corresponds to one layer in a well trained neural net\n",
    "- The betti numbers change in the following way:  \n",
    "$\\beta~(red): (1,2,0) \\rightarrow (1,2,0) \\rightarrow (2,1,0) \\rightarrow (2,0,0) \\rightarrow (1,0,0) \\rightarrow (1,0,0)$  \n",
    "$\\beta~(green): (2,2,0) \\rightarrow (2,2,0) \\rightarrow (2,1,0) \\rightarrow (2,0,0) \\rightarrow (2,0,0) \\rightarrow (1,0,0)$  \n",
    "- In the end we get two disjoint balls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Analysis of Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of our presentation will be about analyzing the topology of some datasets in different layers of deep neural nets and how they change with respect to different activation functions.  \n",
    "This will give a nice inutition of how neural nets work and why you should use certain activation functions and some others not.  \n",
    "For this, we will not focus on non-topological problems, like the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Key questions and findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results, that we will present will give an intuition, to two peculiar mysteries in Deep Learning:\n",
    "- The ReLU activation function generally performs better, than other activation functions, like the Sigmoid or the Hyperbolic Tangent.\n",
    "- Despite the fact, that shallow networks are able to approximate any function arbitrarily well, deeper networks (when trained well, which is not easy) perform better than shallower networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We seek to classify two disjoint manifolds $M_a, M_b \\subset \\mathbb{R}^d$.  \n",
    "- Sample large but finite set of points $T\\subset M_a \\cup M_b$ uniformly and densely. Write $T_i = T \\cap M_i, i \\in \\{a,b\\}$.  \n",
    "- Feedforward NN is given by composition $\\nu = s \\circ f_l \\circ f_{l-1} \\circ \\dots \\circ f_2 \\circ f_1$, where the $f_i$ are the layers of the NN and $s$ is the score function.  \n",
    "- Write $\\nu_j = f_j \\circ \\dots \\circ f_2 \\circ f_1$ to denote the first $j$ layers of the NN.  \n",
    "- Train the network until it correctly classifies all training examples and almost all test examples. We call such a network \"well-trained\".  \n",
    "- Experiments are intended to show the topologies of $\\nu_j(M_a)$ and $\\nu_j(M_b)$ as j runs from 1 to $l$, for different manifolds and network architectures.  \n",
    "- Perform experiments on both simulated datasets, where we know the topology in advance, and real-world data. Real world datasets are more difficult to handle for various reasons, but the most important one for us is, that they have extremely complex topologies in general.  \n",
    "- Thus the experiments on simulated datasets are very extensive and we can then use some real-world datasets to validate our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Generate the simulated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/simulated_data_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three simulated data sets $D-\\mathrm{I}$, $D-\\mathrm{II}$, $D-\\mathrm{III}$:  \n",
    "$D-\\mathrm{I}$ is sampled from a red two dimensional disk ($M_{b}$) with $9$ green disks positioned in it ($M_{a}$).  \n",
    "We have the betti numbers $\\beta~(M_{a}) = (9,0)$ and $\\beta~(M_{b}) = (1,9)$.  \n",
    "$D-\\mathrm{II}$ is sampled from a 3D manifold consisting of $9$ intrlocked solid tori.  \n",
    "We have the betti numbers $\\beta~(M_{a}) = (9,9,0)$ and $\\beta~(M_{b}) = (9,9,0)$.  \n",
    "$D-\\mathrm{III}$ is sampled from a 3D manifold consisting of $9$ red spheres that each enclose a smaller green sphere that each enclose a red ball.  \n",
    "We have the betti numbers $\\beta~(M_{a}) = (9,0,9)$ and $\\beta~(M_{b}) = (18,0,9)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Manifold is sampled uniformly and densely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to examine topology changing effects of:\n",
    "- different activations (ReLU, leaky ReLU ($\\alpha = 0.2$), tanh)\n",
    "- different network depths (4 to 10 layers)\n",
    "- different network widths (6 to 50 neurons per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use softmax as scoring function and every network is trained to zero training error and near zero (~0.01%) test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a figure showing all the network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/training_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an important note on the activation functions:  \n",
    "- ReLU and leakyReLU are both non-homeomorphic maps, but tanh is  \n",
    "- homeomorphic maps cannot reduce betti numbers since they dont't change the topology  \n",
    "- The only reason tanh can actually change the topology, is because in finite-precision arithmetic, tanh can actually take the values -1 and 1 and hence it is not a homeomorphism anymore (artanh is not defined for -1 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Computing Homology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Vietoris-Rips complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't simply use the Euclidean distance to build the VR-complex. Instead we first build the k-nearest-neighbour graph and use the geodesic distance on it, denoted by $\\delta_k$. For each $x_i, x_j \\in X$ the distance $\\delta_k(x_i, x_j)$ is defined by the minimal number of edges between them in the k-nearest neighbour graph. This has the effect of normalizing distances across layers in neural networks, while preserving connectivity of nearest neighbors. This is desirable, because the layers will change geometry quite drastically and this metric is rather robust to geometric changes, but will still reveal the topological ones, that we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our Vietoris-Rips complex depends on two parameters, that we need to set: $k$ for the metric and the usual $\\epsilon$ we need for the VR-complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simulated data we will set parameters, such that the corresponding VR-complex has the same homology as the simulated dataset and then keep these parameters throughout the layers in the neural networks. This means, that we do not have to do persistent homology in every layer of every network, which will save us a lot of time, that we can invest in more experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set $\\epsilon = 1$ and find a $k$, such that the first betti-number is correct, then fix the found $k_\\star$ and proceed to tweak $\\epsilon$, until all the betti-numbers are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors give the following rough time-estimates: The time required to compute even a single Betti-number can be as much as 30 minutes for the output of a $50$-neuron wide layer (i.e. a $50$-dimensional point cloud), while the single computation of persistent homology to obtain $k_\\star$ and $\\epsilon_\\star$ takes about 80 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting numbers for our simulated data sets are:\n",
    "- For $D-\\mathrm{I}$ we get $k_\\star = 14$ and $\\epsilon_\\star = 2.5$\n",
    "- For both $D-\\mathrm{II}$ and $D-\\mathrm{III}$ we get $k_\\star = 35$ and $\\epsilon_\\star = 2.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the Betti-numbers for different combinations of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/parameters.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Densely sample a point cloud from target space.\n",
    "\n",
    "(ii) Train neural networks.\n",
    "\n",
    "(iii) Compute homology (i.e. Betti numbers at each layer of the neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the homology, the authors don't use the entirety of the sampled point cloud, but rather a fraction of about $\\frac{1}{4}$ of the points, that were used as training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/data_set_sizes.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the results of the experiment for $D-\\mathrm{I}$ for the class $M_{a}$.   \n",
    "On the bottom we can the the projection on the first two pricipal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the results of the experiment for $D-\\mathrm{2}$ for the class $M_{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the results of the experiment for $D-\\mathrm{3}$ for the class $M_{a}$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all experiments, ReLU reduces the betti numbers best. Tanh sometimes even increases the betti numbers.  \n",
    "Reducing the $\\beta_{1}$ numbers in the second experiment seems to be the hardest. Tanh really struggles reducing this to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in general we can see that non-homeomorphic maps like ReLU or leakyReLU reduce the betti numbers much faster than a homemorphic map like tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a look at how the width of the network affects the reduction of betti numbers for the class $M_{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the width does not really influence the convergence that much but in general, thinner networks converge a little bit faster but are harder to train. Only the bottleneck network behaves a little bit different. It forces a quick reduction of betti numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we will look at different depths of networks for the class $M_{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_5.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, reducing the depth, makes the change in betti numbers much faster but is concentrated in the final layers and the first layers do not seem to play an important role at all for really small networks. The last layers just have to \"work harder\".  \n",
    "Also training for such small networks gets really hard.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results on real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the datasets MNIST Handwritten Digits and HTRU2 High Time-Resolution Universe Survey.  \n",
    "For these real world datsets, it is not longer possible to train the network near perfectly (a test accuracy of around 95-98% will be achieved).  \n",
    "The topology fo these datasets is also not know before, so the persistent homology will be computed in every layer.  \n",
    "Also only one network (10 layers of width 10) will be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. MNIST Handwritten Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the $70,000$ images in the MNIST handwritten digits data set is an image of size $28 \\times 28$-pixels and collectively forms a point cloud on some manifold $M \\subseteq \\mathbb{R}^{784}$. Computing persistance homology for this space is too hard, so the images will be projected to first 50 principal compontens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results2_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a binarfy classification task, we say the class $M_{a}$ is the class of the number $a$ and $M_{b}$ is the class of non-a numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the results for $a = 0$ (sum of first 3 betti numbers of $M_{a}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results2_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this experiment verify the results of the simulated data.  \n",
    "As you can see, ReLU and leakyReLU reduce betti numbers much faster and tanh even fails to rfeduce the betti numbers to 1 in the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. HTRU2 High Time Resolution Universe Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set consists of statistics of radio source signals from 17,898 stars, measured during the High Time Resolution Universe Survey (HTRU2) experiment to identify pulsars. For our purpose, it suffices to know that pulsars are stars that produce radio emission measurable on earth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture below shows the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results2_3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows 3 different layers for the tanh network and the ReLU network ($M_{a}$ (red) are the pulsar stars and $M_{b}$ (blue) are the non-pulsar stars).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results2_4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results2_5.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are the same as before. But for this dataset even the ReLU network does not reduce $\\beta_{0}$ to 1. Maybe a deeper network would have worked better here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that neural networks indeed work by changing the topology of the given data, reducing its complexits until it becomes linearly seperable.  \n",
    "We have also seen that using a non-homeomorphic map like ReLU works much better for reducing the betti numbers of the data in contrast to tanh and hece yields better performance for smaller nets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
