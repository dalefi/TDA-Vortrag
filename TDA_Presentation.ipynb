{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topology of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the topology of deep neural nets in binary classification tasks:  \n",
    "1) How is a simple neural net built?  \n",
    "2) Binary classification  \n",
    "3) topology of neural nets  \n",
    "4) Analysis of Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural net is given by the composition of functions of the form $f(x) = K(Wx + b)$, together with some scoring function.  \n",
    "With $K$ being some non-linear activation function, the weight matrix $W$ and a bias vector $b$.  \n",
    "In the picture below, each hidden layer corresponds to one of these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/intro_2.png \"Simple neural net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification tasks, we try to say to which of the classes of a given set, a picture corresponds.  \n",
    "In binary classification, we have only two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Topology of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below gives an inuition of what is meant by the topology of a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/intro_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given two disjoint manifolds (green and red)\n",
    "- Each corresponding to a certain class (e.g. red are all cat images and green are all dog images in our train set)\n",
    "- Each step corresponds to one layer in a well trained neural net\n",
    "- The betti numbers change in the following way:  \n",
    "$\\beta~(red): (1,2,0) \\rightarrow (1,2,0) \\rightarrow (2,1,0) \\rightarrow (2,0,0) \\rightarrow (1,0,0) \\rightarrow (1,0,0)$  \n",
    "$\\beta~(green): (2,2,0) \\rightarrow (2,2,0) \\rightarrow (2,1,0) \\rightarrow (2,0,0) \\rightarrow (2,0,0) \\rightarrow (1,0,0)$  \n",
    "- In the end we get two disjoint balls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Analysis of Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of our presentation will be about analyzing the topology of some datasets in different layers of deep neural nets and how they change with respect to different activation functions.  \n",
    "This will give a nice inutition of how neural nets work and why you should use certain activation functions and some others not.  \n",
    "For this, we will not focus on non-topological problems, like the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We seek to classify two disjoint manifolds $M_a, M_b \\subset \\mathbb{R}^d$.  \n",
    "- Sample large but finite set of points $T\\subset M_a \\cup M_b$ uniformly and densely. Write $T_i = T \\cap M_i, i \\in \\{a,b\\}$.  \n",
    "- Feedforward NN is given by composition $\\nu = s \\circ f_l \\circ f_{l-1} \\circ \\dots \\circ f_2 \\circ f_1$, where the $f_i$ are the layers of the NN and $s$ is the score function.  \n",
    "- Write $\\nu_j = f_j \\circ \\dots \\circ f_2 \\circ f_1$ to denote the first $j$ layers of the NN.  \n",
    "- Train the network until it correctly classifies all training examples and almost all test examples. We call such a network \"well-trained\".  \n",
    "- Experiments are intended to show the topologies of $\\nu_j(M_a)$ and $\\nu_j(M_b)$ as j runs from 1 to $l$, for different manifolds and network architectures.  \n",
    "- Perform experiments on both simulated datasets, where we know the topology in advance, and real-world data. Real world datasets are more difficult to handle for various reasons, but the most important one for us is, that they have extremely complex topologies in general.  \n",
    "- Thus the experiments on simulated datasets are very extensive and we can then use some real-world datasets to validate our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Generate the simulated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/simulated_data_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three simulated data sets $D-\\mathrm{I}$, $D-\\mathrm{II}$, $D-\\mathrm{III}$:  \n",
    "$D-\\mathrm{I}$ is sampled from a red two dimensional disk ($M_{b}$) with $9$ green disks positioned in it ($M_{a}$).  \n",
    "We have the betti numbers $\\beta~(M_{a}) = (9,0)$ and $\\beta~(M_{b}) = (1,9)$.  \n",
    "$D-\\mathrm{II}$ is sampled from a 3D manifold consisting of $9$ intrlocked solid tori.  \n",
    "We have the betti numbers $\\beta~(M_{a}) = (9,9,0)$ and $\\beta~(M_{b}) = (9,9,0)$.  \n",
    "$D-\\mathrm{III}$ ios sampled from a 3D manifold consisting of $9$ red spheres that each enclose a smaller green sphere that each enclose a red ball.  \n",
    "We have the betti numbers $\\beta~(M_{a}) = (9,0,9)$ and $\\beta~(M_{b}) = (18,0,9)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Manifold is sampled unifromly densely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ii) Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to examine topology changing effects of:\n",
    "- different activations (ReLU, leaky ReLU ($\\alpha = 0.2$), tanh)\n",
    "- different network depths (4 to 10 layers)\n",
    "- different network widths (6 to 50 neurons per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use softmax as scoring function and every network is trained to zero training error and near zero (~0.01%) test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a figure showing all the network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/training_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an important note on the activation functions:  \n",
    "- ReLU and leakyReLU are both non-homeomorphic maps, but tanh is  \n",
    "- homeomorphic maps cannot reduce betti numbers since they dont't change the topology  \n",
    "- The only reason tanh can actually change the topology, is because in finite-precision arithmetic, tanh can actually take the values -1 and 1 and hence it is not a homeomorphism anymore (artanh is not defined for -1 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (iii) Computing Homology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Building the Vietoris-Rips complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't simply use the Euclidean distance to build the VR-complex. Instead we first build the k-nearest-neighbour graph and use the geodesic distance on it, denoted by $\\delta_k$. For each $x_i, x_j \\in X$ the distance $\\delta_k(x_i, x_j)$ is defined by the minimal number of edges between the in the k-nearest neighbour graph. This has the effect of normalizing distances across layers in neural networks, while preserving connectivity of nearest neighbors. This is desirable, because the layers will change geometry quite drastically and this metric is rather robust to geometric changes, but will still reveal the topological ones, that we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our Vietoris-Rips complex depens on two parameters, that we need to set: $k$ for the metric and the usual $\\epsilon$ we need for the VR-complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simulated data we will set parameters, such that the corresponding VR-complex has the same homology as the simulated dataset and then keep these parameters throughout the layers in the neural networks. This means, that we do not have to do persistent homology in every layer of every network, which will save us a lot of time, that we can invest in more experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set $\\epsilon = 1$ and find a $k$, such that the first betti-number is correct, then fix the found $k_\\star$ and proceed to tweak $\\epsilon$, until all the betti-numbers are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the results of the experiment for $D-\\mathrm{I}$.  \n",
    "On the bottom we can the the projection on the first two pricipal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the results of the experiment for $D-\\mathrm{2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the results of the experiment for $D-\\mathrm{3}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all experiments, ReLU reduces the betti numbers best. Tanh sometimes even increases the betti numbers.  \n",
    "Reducing the $\\beta_{1}$ numbers in the second experiment seems to be the hardest. Tanh really struggles reducing this to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in general we can see that non-homeomorphic maps like ReLU or leakyReLU reduce the betti numbers much faster than a homemorphic map like tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a look at how the width of the network affects the reduction of betti numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the width does not really influence the convergence that much but in general, thinner networks converge a little bit faster but are harder to train. Only the bottleneck network behaves a little bit different. It forces a quick reduction of betti numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we will look at different depths of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results_5.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, reducing the depth, makes the change in betti numbers much faster but is concentrated in the final layers and the first layers do not seem to play an important role at all for really small networks. The last layers just have to \"work harder\".  \n",
    "Also training for such small networks gets really hard.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
