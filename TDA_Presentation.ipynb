{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- neural nets\n",
    "- general idea\n",
    "- structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to classify two disjoint manifolds $M_a, M_b \\subset \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample large but finite set of points $T\\subset M_a \\cup M_b$ uniformly and densely. Write $T_i = T \\cap M_i, i \\in \\{a,b\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward NN is given by composition $\\nu = s \\circ f_l \\circ f_{l-1} \\circ \\dots \\circ f_2 \\circ f_1$, where the $f_i$ are the layers of the NN and $s$ is the score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $\\nu_j = f_j \\circ \\dots \\circ f_2 \\circ f_1$ to denote the first $j$ layers of the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network until it correctly classifies all training examples and almost all test examples. We call such a network \"well-trained\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments are intended to show the topologies of $\\nu_j(M_a)$ and $\\nu_j(M_b)$ as j runs from 1 to $l$, for different manifolds and network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform experiments on both simulated datasets, where we know the topology in advance, and real-world data. Real world datasets are more difficult to handle for various reasons, but the most important one for us is, that they have extremely complex topologies in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the experiments on simulated datasets are very extensive and we can then use some real-world datasets to validate our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (i) Generate the simulated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier n paar nice Bilder einf√ºgen mit Betti-Zahlen. Gibt nicht viel zum Prozess zu sagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ii) Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to examine topology changing effects of:\n",
    "- different activations (ReLU, leaky ReLU, tanh\n",
    "- different network depths (4 to 10 layers)\n",
    "- different network widths (6 to 50 neurons per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (iii) Computing Homology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Building the Vietoris-Rips complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't simply use the Euclidean distance to build the VR-complex. Instead we first build the k-nearest-neighbour graph and use the geodesic distance on it, denoted by $\\delta_k$. For each $x_i, x_j \\in X$ the distance $\\delta_k(x_i, x_j)$ is defined by the minimal number of edges between the in the k-nearest neighbour graph. This has the effect of normalizing distances across layers in neural networks, while preserving connectivity of nearest neighbors. This is desirable, because the layers will change geometry quite drastically and this metric is rather robust to geometric changes, but will still reveal the topological ones, that we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our Vietoris-Rips complex depens on two parameters, that we need to set: $k$ for the metric and the usual $\\epsilon$ we need for the VR-complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simulated data we will set parameters, such that the corresponding VR-complex has the same homology as the simulated dataset and then keep these parameters throughout the layers in the neural networks. This means, that we do not have to do persistent homology in every layer of every network, which will save us a lot of time, that we can invest in more experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set $\\epsilon = 1$ and find a $k$, such that the first betti-number is correct, then fix the found $k_\\star$ and proceed to tweak $\\epsilon$, until all the betti-numbers are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
